<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    div.padded {  
      padding-top: 0px;  
      padding-right: 100px;  
      padding-bottom: 0.25in;  
      padding-left: 100px;  
    }  
  </style> 
<title>Foundations of Ray Tracing</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Foundations of Ray Tracing</h1>
    <h2 align="middle">Seohyun Jeon</h2>

    <div class="padded">
        <h2 align="middle">Overview</h2>
        <p>In this project, I learned why my laptop will never be able to handle ray-tracing in games. The poor thing could hardly render one (1) cow, it would implode if it had to ray trace hundreds of thousands of triangles in an open world game. Guess those will wait until I build a PC :’)</p>
        <p>Overall, we dealt with 3 main sectors of ray tracing. The first involved actual ray generation and intersection testing. This was mostly contained in Part 1. We then dealt with acceleration, as naive ray tracing is extremely slow and costly. The biggest change we made to deal with this issue was implementing BVH trees in order to speed up traversal of primitives when testing for intersection. We later added adaptive sampling in order to minimize the number of samples taken to produce a high quality image. Finally, we dealt with the actual ray tracing: tracing the path of light and calculating the light that reaches our viewpoint.</p>
        <p>We also had to maneuver ourselves around 3 spaces: 2D image space, 3D object space, and 3D world space. We only directly deal with 2D image space when we are tracing our back to our final 2D image. We worked with object space and world space together when tracing rays, as some information is stored in object space coordinates and others are in world space.</p>
        <p>Throughout the project, I came to understand why ray tracing is so costly, but also impressed by the effects that even just a few tracing depth levels can have. It’s also cool to think about how real world light is technically infinite levels of ray tracing that we’re just taking in with our eyes all the time without much thought vs. computers chugging to produce the same result. Thank you GPUs everywhere :)</p>

        <h2 align="middle">Part 1: Ray Generation and Intersection</h2>
        <h3 align="middle">Ray Generation</h3>
        <p>
            To generate a ray given image coordinates, we have to transform image space to camera world to real world. To transform from image space to camera space, I used a simple idea of proportions. An important factor to account for in this transformation is that the center (0.5, 0.5) in image space corresponds to (0, 0, 1) in camera space. Thus, not only are we adding a third dimension and “pushing back” our image to plane <code>z=-1</code>, we are also moving the center.
        </p>
        <p>
            To do this, I calculated the difference between <code>(x,y)</code>and (0.5, 0.5) in image space as a proportion of the dimensions of the quadrant it was in. Because our image space is a 1x1 square, each quadrant had 0.5x0.5 dimensions. I then multiplied this “offset from center” vector by the dimensions of the quadrant in camera space⁠—<code>tan(hFov/2)</code> and <code>tan(vFov/2)</code> respectively to get our direction vector in camera space. Finally, I multiplied this camera vector with our <code>c2w</code> (camera to world) matrix and normalized it to get the final world space ray direction vector.
        </p>
        <h3 align="middle">Intersection</h3>
        <p>
            To implement triangle intersection, I used the Moller-Trumbore algorithm to find the time <code>t</code> at which our ray would intersect the triangle as well as the barycentric coordinates of the point of intersection. If the Moller-Trumbore algorithm returned a valid <code>t</code> as well as barycentric coordinates, I updated the ray’s <code>max_t</code> and calculated the surface normal at the point of intersection with the barycentric coordinates.
        </p>
        
        <h3 align="middle">Normal Shading Results</h3>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/CBspheres.png" width="400px" />
                        <figcaption align="middle">They seem to be having a serious talk</figcaption>
                    </td>
                    <td>
                        <img src="./images/blob.png" width="400px" />
                        <figcaption align="middle">A blob. Abstract art?</figcaption>
                    </td>

                </tr>
            </table>
        </div>


        <h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
        <h3 align="middle">Constructing a BVH Tree</h3>
        <p>I used recursion to implement BVH construction. Regardless of node type, I first gathered every primitive from start to end into a bounding box and new node. After this, if the number of items in the newly constructed node was less than or equal to the max leaf size, I returned this node as a leaf after setting appropriate values. If not, I found the largest axis and split it at the average centroid on that axis. I then looped through the primitives and split into left and right nodes depending on whether the given primitive’s centroid was less than or greater than the average centroid on the chosen axis. I used average centroid instead of midpoint in order to split the primitives more evenly, as average centroid captures some of the skew of primitive distribution along the longest axis. I then recursively split each left/right node until the BVH was fully constructed.</p>

        <h3 align="middle">Gotta Go Fast</h3>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/ocownobvh.png" width="400px" />
                        <figcaption align="middle">Pre-BVH stats...</figcaption>
                    </td>
                    <td>
                        <img src="./images/ocowbvh.png" width="400px" />
                        <figcaption align="middle">Post-BVH stats!</figcaption>
                    </td>

                </tr>
            </table>
        </div>
        <p>
            Here, we can see that rendering <code>cow.dae</code> took ~72 seconds to render without BVH. Not terrible, but cow.dae is one of the simpler meshes. This will fall apart quickly. After implementing BVH, we can see that rendering the cow takes less than a second--only 0.08 seconds! Without BVH, we were naively checking if a ray intersected with any of the primitives in the mesh. On the other hand, with BVH, we are able to skip checking anything in a bounding box that does not intersect with our ray since it is impossible for a ray to intersect with a primitive inside a bounding box it does not intersect. Because of this, we save massive amounts of time in checking ray intersection. Because of our tree structure, we can easily traverse down to the leaf that our ray intersects with if an intersection exists. This is clear when we see that we were averaging 2700+ intersection tests per ray without BVH vs. about 2.5 per ray with BVH!
        </p>
        <h3 align="middle">So Many Triangles</h3>
        <p>Below are some meshes that can only feasibly be rendered after implementing BVH! </p>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/cblucy.png" width="400px" />
                        <figcaption align="middle">CBlucy... Cool Bro Lucy</figcaption>
                    </td>
                    <td>
                        <img src="./images/planck.png" width="400px" />
                        <figcaption align="middle">Mood</figcaption>
                    </td>

                </tr>
            </table>
        </div>

        <h2 align="middle">Part 3: Direct Illumination</h2>
        <h3 align="middle">Uniform Hemisphere Sampling</h3>
        <p>For uniform hemisphere sampling, I took num_samples number of samples in order to estimate the irradiance at our given hit point hit_p. For each sample, I first used <code>UniformHemisphereSampler3D::get_sample()</code> to get an object-space vector wi_object in the hemisphere. I then created a new ray with origin <code>hit_p</code> and direction <code>wi_world</code>. I set the <code>min_t</code> of this ray to <code>EPS_F</code> in order to avoid collision with the surface hit at <code>hit_p</code>. I then checked if this ray intersected with any primitives in my bvh, and added the emission weighted by the BSDF of our hit and the cosine of our <code>wi</code> vector. After accumulating all of these samples, I divided by the uniform pdf as well as <code>num_samples</code> to get an average irradiance.</p>

        <h3 align="middle">Importance Sampling</h3>
        <p>
            For importance sampling, we directly sample the lights instead of uniformly in the hemisphere. To implement this, I looped over all of the lights in <code>scene->lights</code>. For each light, if the light was a delta light, I only sampled once. For non-delta lights, I sampled <code>num_samples</code> number of times.
        </p>
        <p>
            For each sample, I used <code>SceneLight::sample_L</code> to get an emission for the given hit point and light as well as world space vector <code>wi</code> from hit point to light source, the distance, and pdf. If this light is behind the surface at the hit point, we skip this sample. After this, I created a ray from our hit point in the sampled direction <code>wi</code>. Again, we set <code>min_t</code> of this ray to <code>EPS_F</code> and offset the distance to the light by <code>EPS_F</code> as well to avoid collision with our hit surface and the light itself. If nothing intersects this ray, we know that there is nothing between the light source and our hit point, so we weight and add the emission from this point to a cumulative sum for each light. Once we finish sampling one light, we normalize the sum by the number of samples and add it to our total light for the hit point.
        </p>

        <h3 align="middle">Uniform vs Lighting</h3>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/64bunnyhemi.png" width="400px" />
                        <figcaption align="middle">Uniform hemisphere sampling. That's rough, buddy.</figcaption>
                    </td>
                    <td>
                        <img src="./images/64bunnyimport.png" width="400px" />
                        <figcaption align="middle">Lighting sampling</figcaption>
                    </td>
                </tr>
                <tr>
                    <td>
                        <img src="./images/64spherehemi.png" width="400px" />
                        <figcaption align="middle">Uniform hemisphere sampling</figcaption>
                    </td>
                    <td>
                        <img src="./images/64sphereimp.png" width="400px" />
                        <figcaption align="middle">Lighting sampling</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>We can see that using lighting sampling produces a much smoother, less noisy image! This is because every ray we sample is "relevant" in a sense. In uniform hemisphere sampling, we sample in every direction from our hit point, few of which actually connect to a light source. We are thus able to converge to the "true" image much faster than if we were to take uniform samples until enough relevant rays were produced.</p>

        <h3 align="middle">Number of Light Rays vs. Noise</h3>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/bunny_1_1.png" width="200px" />
                        <figcaption align="middle">1 light ray</figcaption>
                    </td>
                    <td>
                        <img src="./images/bunny_1_4.png" width="200px" />
                        <figcaption align="middle">4 light rays</figcaption>
                    </td>
                    <td>
                        <img src="./images/bunny_1_16.png" width="200px" />
                        <figcaption align="middle">16 light rays</figcaption>
                    </td>
                    <td>
                        <img src="./images/bunny_1_64.png" width="200px" />
                        <figcaption align="middle">64 light rays</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>Each image above was rendered using 1 sample per pixel and light sampling. We can see that increasing the number of samples per area light reduces the noise in our output. When looking specifically at the soft shadow that the bunny casts on the floor, we can see that its area converges closer to the actual shadow, and the transition becomes much smoother.</p>

        <h2 align="middle">Part 4: Global Illumination</h2>
        <p>At this point, we've implemented direct illumination; light <em>at</em> light sources and light <em>directly from</em> light sources. Now we need to implement the costly part of ray tracing; calculating indirect light. Light bounces, so all of our images should be much brighter. For example, the light that hits the bunny directly from the ceiling light will bounce and illuminate its soft shadow, and light bouncing from the walls will hit the bunny and brighten it while also adding a hue.</p>

        <h3 align="middle">Indirect Lighting</h3>
        <p>To implement indirect lighting, we need to create functionality that allows us to measure light that hits our point of interest that isn’t necessarily directly from a light source. This is done by recursively looping through <code>at_least_one_bounce_radiance</code></p>
        <p>First, we take the one_bounce radiance of our given ray. This is the baseline radiance we will return since we are trying to accumulate the light from at least one bounce. After this, we check if the depth of our ray is less than 1, meaning we have reached our max ray depth. If so, we simply return the single bounce radiance.</p>
        <p>If the depth is not less than 1, we then use russian roulette to avoid infinite recursion. I chose termination probability 0.4, which means that there is a 0.4 chance of our recursion ending despite not reaching maximum depth. If we make it past the russian roulette, we then proceed to sample the source of our current hit point (which, unlike direct lighting, may not be a light source). If our new ray with origin at the current hit point and direction sampled from the original intersection intersects any primitive in our bvh, we recursively call <code>at_least_one_bounce_radiance</code>. We weigh the radiance derived from the recursive call the same way as in direct illumination, except we also divide by the continuation probability, which is 0.6 in my implementation.</p>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/buns1024l1.png" width="400px" />
                        <figcaption align="middle">Bunny, but brighter now :)</figcaption>
                    </td>
                    <td>
                        <img src="./images/speero1024.png" width="400px" />
                        <figcaption align="middle">Their talk seems to be going well...</figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <p>Both images above were rendered with 1024 samples/pixel, 1 light ray, and max ray depth 5. We can see that both images are much brighter than with only direct illumination. We can especially see in the spheres that the red and blue of the walls is reflected back onto the spheres and their shadows. The shadows are much lighter as well now, since they receive some indirect light, whereas any surface not directly hit by light was completely dark previously.</p>

        <h3 align="middle">A Tale of Two <strike>Cities</strike> Lighting Methods...?</h3>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/sphereonlydirect.png" width="400px" />
                        <figcaption align="middle">Only direct lighting. Looks familiar!</figcaption>
                    </td>
                    <td>
                        <img src="./images/sphereonlyindirect.png" width="400px" />
                        <figcaption align="middle">Only indirect lighting...mood lighting?</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>On the left, we have our spheres but only with direct lighting. This is essentially what we implemented in part 3, with only zero-bounce and one-bounce lighting. On the other hand, only indirect lighting is everything <em>other</em> than zero-bounce and one-bounce lighting. The only light in the image on the right is bounced from a non-light source. We can see that the most indirect illumination comes from the second bounce; for example, the indirect lighting image is brightest at the points on the sphere closer to the walls, which reflect direct light on the second bounce. We can think of this as a breakdown of what we've added with Part 4! If you add the illumination from these two images, we get our globally illuminated image like above.</p>

        <h3 align="middle">Max Depth</h3>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/bunzerodepth.png" width="300px" />
                        <figcaption align="middle">Max depth 0</figcaption>
                    </td>
                    <td>
                        <img src="./images/buns1024l1m0.png" width="300px" />
                        <figcaption align="middle">Max depth 1</figcaption>
                    </td>
                    <td>
                        <img src="./images/buns1024l1m1.png" width="300px" />
                        <figcaption align="middle">Max depth 2</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/buns1024l1m2.png" width="300px" />
                        <figcaption align="middle">Max depth 3</figcaption>
                    </td>
                    <td>
                        <img src="./images/buns1024l1m100.png" width="300px" />
                        <figcaption align="middle">Max depth 100</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>Above, we can see that max depth 0 produces the same image as zero-bounce lighting, and max depth 1 produces the same image as one-bounce lighting. Our depth essentially dictates how many times we trace our light "bouncing". We see that there is a significant change from depths 1 to 2 (and 0 to 1 obviously). However, the differences are much subtler from depth 2 to 3, and almost unnoticeable from 3 to 100. This shows the importance of setting a conservative max depth and using russian roulette to stop recursion. The additional benefit/realism we get from 97 extra bounces is negligible and not worth the additional computational cost.</p>

        <h3 align="middle">Various Sample-per-pixel Rates</h3>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td>
                        <img src="./images/spheres1l4.png" width="200px" />
                        <figcaption align="middle">1 sample/pixel</figcaption>
                    </td>
                    <td>
                        <img src="./images/spheres2l4.png" width="200px" />
                        <figcaption align="middle">2 samples/pixel</figcaption>
                    </td>
                    <td>
                        <img src="./images/spheres4l4.png" width="200px" />
                        <figcaption align="middle">4 samples/pixel</figcaption>
                    </td>
                    <td>
                        <img src="./images/spheres8l4.png" width="200px" />
                        <figcaption align="middle">8 samples/pixel</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align=align>
                        <img src="./images/spheres16l4.png" width="200px" />
                        <figcaption align="middle">16 samples/pixel</figcaption>
                    </td>
                    <td>
                        <img src="./images/spheres64l4.png" width="200px" />
                        <figcaption align="middle">64 samples/pixel</figcaption>
                    </td>
                    <td>
                        <img src="./images/spheres1024l4.png" width="200px" />
                        <figcaption align="middle">1024 samples/pixel</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>Each image above was generated with 4 light rays and max ray depth 5. We can see that low sample/pixel rates produce very grainy/noisy images when using global illumination. Even at 64 samples, the noise is noticeable, and it takes 1024 samples to reach a generally smooth image. Since the light calculations are far more complex for global illumination, it takes a much higher sampling rate per pixel to converge to the true image.</p>


        <h2 align="middle">Part 5: Adaptive Sampling</h2>
        <p>To implement adaptive sampling, I made a few simple changes to <code>raytrace_pixel</code> from Part 1. Every <code>samplesPerBatch</code> sample I took for the pixel, I checked for convergence. To do this, I kept track of the cumulative sum of illumination as well as the cumulative sum of illumination squared. With these two statistics, I can calculate the mean and standard deviation of samples so far, and in turn check if these statistics have satisfied our stopping condition, dependent on <code>maxTolerance</code>. If the convergence condition was satisfied, I stopped sampling the given pixel. To produce a sampling rate visualization, I kept track of the sampling rate for each pixel in <code>sampleCountBuffer</code>.</p>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                        <img src="./images/bunnyadepti.png" width="480px" />
                        <figcaption align="middle">2048 samples/pixel</figcaption>
                    </td>
                    <td align="middle">
                        <img src="./images/bunnyadepti_rate.png" width="480px" />
                        <figcaption align="middle">Sampling rate visualization</figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <p>Thanks to adaptive sampling, we can generate images with a whopping 2048 samples per pixel without frying our machines! This is because not every pixel (in fact, the majority) is actually sampled a full 2048 times. In the sampling rate visualization on the right, red areas have high sampling rates, and blue areas have low sampling rates.</p>

    </div>
    <div class="padded">

        

            <h2 align="middle">Part 6: Mirror and Glass Materials</h2>
            <p>
            Originally, we were only able to render diffuse surfaces that bounced light equally in all directions. This time, we implemented mirror surfaces that perfectly reflect light, glass that both reflects and refracts, and microfacet surfaces that have both color and reflective properties!
        </p>
        <p>
            While diffuse surfaces bounce light in equal directions, sampling is very different for the materials in this part of the project. Rays hitting perfect mirrors have one exact reflection, and rays hitting refractive surfaces have exactly one refracted ray. Microfacet materials, on the other hand, can be thought of as materials with countless tiny bumps on the surface, each being a mirror itself. Additionally, microfacet materials have different colors (as seen in our gold dragon vs copper bunny later on) due to the differing reflections at RGB wavelengths.
        </p>
            <h3 align="middle">Mirror Materials</h3>
            <p>
                To implement mirror materials, we need to implement a new sampling method, <code>MirrorBSDF::sample_f()</code> that emulates light bouncing off of a perfect mirror. To do this, we implement <code>BSDF::reflect()</code>, which takes in outgoing ray <code>wo</code> and populates the incoming vector <code>wi</code>. Because the intersection point in BSDF calculations is the origin of the space our rays our defined in, the mirror reflection of <code>wo</code> is simply the reflection over the normal. Fortunately, the normal is aligned with the z axis in our BSDF coordinate space, so <code>wi</code> has the same <code>x</code>, <code>y</code>, <code>z</code> values as <code>wo</code>, except with the signs flipped for <code>x</code> and <code>y</code>.
            </p>
            <p>
                In <code>MirrorBSDF::sample_f()</code>, we simply use <code>reflect()</code> to appropriately populate <code>wi</code>, and set the <code>pdf</code> to <code>1.0</code> since we are dealing with a delta BSDF and there is no probability involved with sampling; each ray has one exact reflection. Now our incoming ray and pdf are correctly populated. We finally return the reflectance scaled down by the cosine of our incoming ray in order to cancel out the cosine that <code>at_least_one_bounce_radiance</code> uses to emulate Lambertian falloff.
            </p>
            <h3 align="middle">Refractive Materials</h3>
            <p>
                Similarly to above with mirror materials, we must implement a function that samples and populates the incoming ray through a refractive material. First, we define &eta;, the ratio of old index of refraction to new index of refraction. Because we are only dealing with refractive materials and air, this will be a ratio between <code>1.0</code> (index of refraction of air) and our <code>ior</code>. If <code>wo</code> is entering the object in question, then &eta; is defined as <code>1.0/ior</code> since our “old” material is the air, and we enter the “new” material. If we are exiting the object, &eta; is simply <code>ior</code>. We can check whether <code>wo</code> is entering or exiting based on its <code>z</code> coordinate; a positive <code>z</code> indicates that we are entering the refractive material.
            </p>
            <p>We now have all the elements needed to calculate <code>wi</code>. We can extrapolate from Snell’s law to conclude that <code>cos&theta;' = sqrt(1-&eta;<sup>2</sup>(1 - &omega;<sub>o</sub>z<sup>2</sup>))</code>, where <code>&theta;'</code> is the angle between the normal and <code>wi</code>. However, if the term under the square root is negative and we don’t have a real root for <code>cos&theta;'</code>, there is total internal reflection and no light is reflected to the new material. Thus, we return false and leave <code>wi</code> empty in this case. If we have a valid refraction, we populate <code>wi</code> appropriately using our known &eta; and <code>wo</code>.</p>
            <p>Finally, we put this all together in <code>RefractionBSDF::sample_f()</code>. If there is a valid refraction (not total internal reflection) we use <code>refract()</code> to populate <code>wi</code>. Again, <code>pdf</code> is simply <code>1.0</code>. We then return transmittance scaled down by cosine of our incoming ray for the same reason as in mirror materials. We also scale down by &eta;<sup>2</sup> to emulate the level of dispersion dependent on the index of refraction.</p>
            <h3 align="middle">Porque no los dos? Glass!</h3>
            <p>At this point, we can render purely reflective and refractive materials. We can see this in the image below, with the left sphere being purely reflective and the right sphere being purely refractive. However, glass both refracts <i>and</i> reflects light; we can see our reflection in glass windows! To realistically render glass, we need to create a hybrid of these two extremes.</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/testoto.png" width="480px" />
                            <figcaption align="middle">Reflective vs refractive sphere</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>
                If total internal reflection occurs, we simply treat this case as a reflection. If not, we need to calculate Schlick’s approximation <code>R</code>, the proportion of light that is reflected. We flip a coin with probability <code>R</code> of returning true; this means that approximately fraction <code>R</code> of our valid light rays will be reflected. Over a large number of samples/rays, this will have the same effect as each individual ray being split into reflection and refraction. If our coin flip yields true, we set <code>pdf</code> to <code>R</code>, the probability that we reflect the given ray. We then return the appropriately scaled reflectance pased on the pdf and reflectance. Else, we set <code>pdf</code> to <code>1.0-R</code> and return the appropriately scaled transmittance.
            </p>
            <h3 align="middle">Mirror, Mirror on the Wall</h3>
            <p>All images below are rendered with 64 samples per pixel and 4 samples per light</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/depth0.png" width="400px" />
                            <figcaption align="middle">Depth = 0</figcaption>
                        </td>
                        <td align="middle">
                            <img src="./images/depth1.png" width="400px" />
                            <figcaption align="middle">Depth = 1</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>At depth 0, we simply have zero bounce illumination, and can only see the light source. At depth 1, we get one bounce illumination. The spheres are not lit up the same way our diffuse spheres were lit up with one bounce illumination in part 1, since our sampling method is different. We can also see that the reflection on the right (refractive) sphere has many black spots, because those spots were rays that were refracted (and can’t reach the camera with depth 1).</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/depth2.png" width="400px" />
                            <figcaption align="middle">Depth = 2</figcaption>
                        </td>
                        <td align="middle">
                            <img src="./images/depth3.png" width="400px" />
                            <figcaption align="middle">Depth = 3</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>At depth 2, the mirror sphere starts coming into view, as light bounces off the walls and onto the surface of the sphere. However, the glass ball is still quite dark as the refracted light has only been able to enter the glass sphere, not exit. At depth 3, the glass sphere also comes into view, as light rays have hit, entered, and exited the glass. However, the glass ball is still dark in the mirror reflection because we need an additional bounce to bounce the light onto the mirror.</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/depth4.png" width="400px" />
                            <figcaption align="middle">Depth = 4</figcaption>
                        </td>
                        <td align="middle">
                            <img src="./images/depth5.png" width="400px" />
                            <figcaption align="middle">Depth = 5</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>At depth 4, all essential lighting is achieved, as the glass sphere is visible in the reflection of the mirror sphere. This is possible because we have traced the ray from the reflection to the surface of the glass, into the glass, out of the glass, and to the light source. At depth 5, we simply get a slightly brighter image from more bounces.</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/depth100.png" width="480px" />
                            <figcaption align="middle">Depth = 100</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>Depth 100! A slightly brighter image from more bounced light. However, the difference between depth 5 and 100 is quite small, as the noticeable light bouncing occurs within the first handful of levels.</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/spheres.gif" width="480px" />
                            <figcaption align="middle">Emerging from the darkness!</figcaption>
                        </td>
                    </tr>
                </table>
            </div>

            <h2 align="middle">Microfacet Material</h2>
            <h3 align="middle">BRDF Evaluation Function</h3>
            <p>The microfacet BRDF evaluation function is given by the equation below, where F is the Fresnel term, G is the shadow-masking term, D is the normal distribution function, n is the surface normal, and h is vector halfway between <code>wo</code> and <code>wi</code>.</p>

            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/brdf.png" width="300px" />
                        </td>
                    </tr>
                </table>
            </div>
            <p>
                For the Fresnel term, we simplify the process by only calculating the terms for fixed red, green, and blue wavelengths. The Fresnel terms are dependent on &eta; and <i>k</i> values, which are characteristic of different materials. For D, we use the Beckmann distribution, which is dependent on &alpha;, the roughness of our surface, and the angle between our bisecting vector and the normal.
            </p>

            <h3 align="middle">Importance Sampling</h3>
            <p>While cosine hemisphere sampling is not <i>wrong</i>, it is very inefficient for microfacet materials, which do not reflect light uniformly, and will take a high number of samples to converge to the “true” image.</p>

            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/cubunshemi.png" width="400px" />
                            <figcaption align="middle">Cosine hemisphere sampling</figcaption>
                        </td>
                        <td align="middle">
                            <img src="./images/cubunsimport.png" width="400px" />
                            <figcaption align="middle">Importance sampling</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
        <p>Both images above were rendered with 64 samples per pixel, 1 sample per light, and max depth 5. We can see that despite having the same number of samples and depth, the importance sampled bunny is far less noisy than the hemisphere sampled one. This is because cosine sampling leads us to sample many "useless" rays, since microfacet surfaces do not reflect light uniformly the way diffuse surfaces do.</p>

            <h3 align="middle">A new species?</h3>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/crbuns.png" width="480px" />
                            <figcaption align="middle">Chromium bunny!</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>While the original bunny was a copper bunny, this one is a chromium bunny! This was achieved by using &eta; values 3.1743, 3.1800, 2.4650 and <i>k</i> values 3.3000, 3.3300, 3.2150 for R, G, B wavelengths respectively. I found these values using the website linked in the spec.</p>

            <h3 align="middle">Smooth, Smoother, Smoothest</h3>
            <p>All images below (except for the last, see caption) were rendered with 128 samples per pixel, 1 sample per light, and max depth 6.</p>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/dragon5.png" width="300px" />
                            <figcaption align="middle">&alpha; = 0.5</figcaption>
                        </td>
                        <td align="middle">
                            <img src="./images/dragon25.png" width="300px" />
                            <figcaption align="middle">&alpha; = 0.25</figcaption>
                        </td>

                    </tr>
                    <tr>
                        <td align="middle">
                            <img src="./images/dragon05.png" width="300px" />
                            <figcaption align="middle">&alpha; = 0.05</figcaption>
                        </td>
                        <td align="middle">
                            <img src="./images/dragon005.png" width="300px" />
                            <figcaption align="middle">&alpha; = 0.005.. almost as smooth as my brain!</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <div align="center">
                <table style="width=100%">
                    <tr>
                        <td align="middle">
                            <img src="./images/dragon0052.png" width="300px" />
                            <figcaption align="middle">&alpha; = 0.005, 4 samples/light</figcaption>
                        </td>
                    </tr>
                </table>
            </div>
            <p>The &alpha; value corresponds to the roughness of the material; higher values are rougher, while lower values are smoother and thus glossier. We can see that using &alpha; value 0.5 gives us a relatively dull surface. Decreasing this to 0.25 gives us a smoother, more reflective surface. Further decreasing this to 0.05 gives us an extremly shiny and smooth surface that seems to act like a gold-tinged mirror. Finally, at 0.005, we expect an extremely smooth surface. However, there is some aliasing due to the low sampling rate that caused the highlights to not appear prominently. I included another image with 4 samples per light instead of 1 sample per light in order to give a more accurate representation of the material.</p>
        </o></div>
</body>
</html>




